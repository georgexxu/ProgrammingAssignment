{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e27b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "pi = torch.tensor(np.pi,dtype=torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db2ff3",
   "metadata": {},
   "source": [
    "### 1. Gradient descent for the finite neuron method: $L^2$-fitting\n",
    "\n",
    "$$\\sum_{i=1}^n a_i \\sigma(\\omega_i x + b_i)$$\n",
    "\n",
    "1.1 Neural network architecture \n",
    "\n",
    "1.2 Access and modify parameters in a neural network \n",
    "\n",
    "1.3 Define a loss function\n",
    "    \t\\begin{equation} \\label{snn-optimization}\n",
    "\t\t\t \\min_{a_i, b_i} \\int_{-\\pi}^{\\pi} \\frac{1}{2} |f(x) - \\sum_{i=1}^{n} a_i \\sigma( x + b_i) |^2 dx.  \n",
    "\t\t\\end{equation}\n",
    "        \n",
    "1.4 Create an optimizer in pytorch and tune it parameters \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc1b0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e6a7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 Neural network architecture \n",
    "class model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) # W1, b\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False) # W2\n",
    "\n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x)))\n",
    "        return u1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941a423",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2 Access and modify parameters in a neural network \n",
    "\"\"\"\n",
    "Instantiate a class object my_model\n",
    "Access the parameters\n",
    "Mutate(modify) the parameters\n",
    "\"\"\"\n",
    "hidden_layer = 4\n",
    "my_model = model(1,hidden_layer,1)\n",
    "\n",
    "\"\"\"\n",
    "1. Access parameters using my_model.parameters()\n",
    "2. torch parameters contain two attributes: data and requires_grad\n",
    "\"\"\"\n",
    "for parameter in my_model.parameters():\n",
    "    print(parameter.data)\n",
    "    print(parameter.requires_grad)\n",
    "print()\n",
    "\n",
    "\"\"\"\n",
    "1. Directly access the class attributes: fc1, fc2\n",
    "2. Access the paramters as attributes of fc1, fc2\n",
    "\"\"\"\n",
    "print(my_model.fc1.weight)\n",
    "print(my_model.fc1.bias)\n",
    "print(my_model.fc1.weight.data)\n",
    "print(my_model.fc1.weight.requires_grad)\n",
    "print()\n",
    "print(\"+++++++++++++\")\n",
    "for param in my_model.fc1.parameters():\n",
    "    print(param)\n",
    "    param.requires_grad = False \n",
    "    param.requires_grad_()\n",
    "print(my_model.fc1.weight)\n",
    "print(my_model.fc1.bias)\n",
    "\n",
    "\"\"\"\n",
    "Modify the parameter:\n",
    "Shape matches; modify data and requires_grad separately\n",
    "\"\"\"\n",
    "print(\"=========\")\n",
    "w_i = torch.full((1,hidden_layer),1.0).view(hidden_layer,1)\n",
    "print(my_model.fc1.weight.data)\n",
    "my_model.fc1.weight.data = w_i # w_i is now the data\n",
    "my_model.fc1.weight.requires_grad = False \n",
    "print(my_model.fc1.weight)\n",
    "print(my_model.fc1.weight.data)\n",
    "print(my_model.fc1.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71b47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.3 Define a loss function using piecewise Gauss quadrature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a0ac6",
   "metadata": {},
   "source": [
    "#### Standard p-point Gaussian quadrature rule on $[-1,1]$\n",
    "\\begin{equation}\n",
    "    \\int_{-1}^{1} f(x) dx \\approx \\sum_{i = 1}^p w_i f(g_i).\n",
    "\\end{equation}\n",
    "\n",
    "On an arbitrary interval $[x_i, x_{i+1}]$: \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "        \\int_{x_i}^{x_{i+1}} f(x) dx & = \\frac{x_{i+1} - x_{i}}{2 } \\int_{-1}^1 f(\\frac{x_{i+1} - x_i}{2} \\xi + \\frac{x_{i+1} + x_i}{2}) d\\xi \\\\\n",
    "        & \\approx \\frac{x_{i+1} - x_{i}}{2 } \\sum_{j =1}^p w_j f(\\frac{x_{i+1} - x_i}{2}  g_j + \\frac{x_{i+1} + x_i}{2}) \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "**Method**\n",
    "- Divide the domain (interval) into several subdomains (subintervals). $[x_0,x_1], [x_1,x_2],...,[x_{N-1}, x_{N}]$\n",
    "- Compute the quadrature value in each subdomain.\n",
    "- Sum the quadrature values in each subdomain to get the quadrature value over the whole domain.\n",
    "- vectorization \n",
    "\n",
    "**Remark**\n",
    "- Implementing a monte carlo or trapezoidal rule is eaiser but not as accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ee8e8",
   "metadata": {},
   "source": [
    "##### Speed up by vectorization? \n",
    "\n",
    "1. on each subinterval $[x_i, x_{i+1}]$\n",
    "    - $\\frac{x_{i+1} - x_{i}}{2 } \\sum_{j =1}^p w_j f(\\frac{x_{i+1} - x_i}{2}  g_j + \\frac{x_{i+1} + x_i}{2}) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de59417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e9f7432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.6612)\n"
     ]
    }
   ],
   "source": [
    "### 5 point Gauss Quadrature rule\n",
    "\n",
    "def integrand(x):\n",
    "    return torch.sin(10*x)+1\n",
    "\n",
    "gx = torch.tensor([-0.9061798459386639927976, -0.5384693101056830910363, 0, 0.5384693101056830910363, \n",
    "     0.9061798459386639927976]).to(device)\n",
    "gx = gx.view(1,-1)\n",
    "gw = torch.tensor([0.2369268850561890875143, 0.4786286704993664680413, 0.5688888888888888888889, 0.4786286704993664680413,\n",
    "     0.2369268850561890875143]).to(device)\n",
    "gw = gw.view(-1,1) # Column vector \n",
    "num_points = 1 # subintervals\n",
    "nodes = torch.linspace(-pi,pi,num_points+1).view(-1,1) \n",
    "coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "integration_points = coef1@gx + coef2_expand\n",
    "integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "gw_expand = torch.tile(gw,(num_points,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "# Modify coef1 to be compatible with func_values\n",
    "coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "\n",
    "func_values = integrand(integration_points)\n",
    "\n",
    "integral_value = torch.sum(func_values*gw_expand*coef1_expand)\n",
    "print(integral_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0155c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a02941d9",
   "metadata": {},
   "source": [
    "##### 1.4 Create an optimizer in pytorch and tune it parameters \n",
    "- import torch.optim as optim\n",
    "- link: https://pytorch.org/docs/stable/optim.html\n",
    "- Schedule learning rate\n",
    "\n",
    "- Basic syntax:\n",
    "\n",
    "        optimizer.zero_grad() # clear the gradient wrt parameters \n",
    "    \n",
    "        output = model(input) \n",
    "    \n",
    "        loss = loss_fn(output, target) # define the loss \n",
    "    \n",
    "        loss.backward() # compute the gradients \n",
    "    \n",
    "        optimizer.step() # gradient step: $\\theta^{k+1} = \\theta^k - \\nabla_\\theta L(\\theta^k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.4 Create an optimizer in pytorch and tune it parameters \n",
    "\n",
    "# Define optimizer \n",
    "optimizer1 = optim.SGD(my_model.parameters(), lr=0.02)\n",
    "\n",
    "# Learning rate schedule\n",
    "lr = 0.02\n",
    "for param_group in optimizer1.param_groups:\n",
    "    param_group['lr'] = lr * (0.98 ** ((epoch + 1) // 1000)) # Learning rate schedule\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac8a7b",
   "metadata": {},
   "source": [
    "## Orthogonal greedy algorithm for finite neuron method: $L^2$-fitting\n",
    "\n",
    "**How to implement the two major steps in OGA?**\n",
    "\n",
    "1. Solve the argmax problem $\\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle | $ \n",
    "    - $L^2$ innner product: numerical integration \n",
    "        - Piecewise Gauss quadrature (very accurate numerical quadrature is needed for this part)\n",
    "    - **Method of exhaustion** for 1D for a good initial guess \n",
    "        - $\\mathbb{D}_N = \\{ \\sigma(x + b_i), b_i = \\pi -  2 \\pi\\frac{i-1}{N} \\}_{i = 1}^N$\n",
    "        - $\\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D_N}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle | $ \n",
    "            - store values of $g$ as a row vector, values of $f - f_{n-1}$ column vector\n",
    "    - Further optimization using Newton's method to get a better $g_n$ (This part is also essential when the number of neuron is large)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80bf3532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1])\n",
      "torch.Size([100, 50])\n",
      "torch.Size([100, 1])\n",
      "tensor(99)\n"
     ]
    }
   ],
   "source": [
    "## 1. ReLU dictionary discretization \n",
    "# relu dictionary\n",
    "def relu_dict(x_l,x_r,N):\n",
    "\n",
    "    relu_dict_parameters = torch.zeros((N,2))\n",
    "    relu_dict_parameters[:N,0] = torch.ones(N)[:]\n",
    "    relu_dict_parameters[:N,1] = torch.linspace(x_l,x_r,N+1)[1:] # relu(x+bi)\n",
    "\n",
    "    return relu_dict_parameters\n",
    "def target(x):\n",
    "    return x \n",
    "relu_dict_parameters = relu_dict(-pi,pi,100)\n",
    "integration_intervals = 10 \n",
    "nodes = torch.linspace(-pi,pi,integration_intervals+1).view(-1,1) \n",
    "coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "integration_points = coef1@gx + coef2_expand\n",
    "integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "gw_expand = torch.tile(gw,(integration_intervals,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "func_values = target(integration_points) \n",
    "\n",
    "weight_func_values = func_values*gw_expand*coef1_expand\n",
    "print(weight_func_values.size())\n",
    "basis_values = F.relu(relu_dict_parameters[:,0] *integration_points + relu_dict_parameters[:,1]).T # uses broadcasting  \n",
    "print(basis_values.size())\n",
    "output = torch.abs(torch.matmul(basis_values,weight_func_values)) # \n",
    "print(output.size())\n",
    "neuron_index = torch.argmax(output.flatten())\n",
    "\n",
    "print(neuron_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a177d8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41255a2a",
   "metadata": {},
   "source": [
    "2. Orthogonal projection. Given $H_n = \\text{span}\\{g_1, g_2,...,g_n \\}$\n",
    "    - Use w_list, b_list to keep track of the added neurons\n",
    "        - reconstruct the neural network from the two lists \n",
    "    - Solve the linear system: \n",
    "         - $<f_n,g_i> = <f,g_i>$ for all $i= 1,2,...,n$, with $f_n = \\sum_{i=1}^n a_i g_i $\n",
    "         - A liner system in $\\alpha := (a_1, a_2,...,a_n)^T$. $M \\alpha = b$, where $M_{i,j} = <g_j,g_i>$ $b_i = <f,g_i> $\n",
    "         - Equivalently, an $L^2$-fitting: $\\min_{a_1, a_2,...,a_n} \\frac{1}{2} \\|\\sum_{i=1}^n a_i g_i  - f \\|^2_{L^2} $, that is $\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2} \\alpha^T M \\alpha - b^T \\alpha $\n",
    "    - A shortcut: make use of pytorch automatic differentiation to get the matrix M, since we already know how to compute the loss function very accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f06bf8",
   "metadata": {},
   "source": [
    "#### Extract the linear system using pytorch auto differentiation and solve the linear system \n",
    "\n",
    "$L(x) = \\frac{1}{2}x^T A x$\n",
    "\n",
    "$\\nabla_x L(x) = A x$\n",
    "\n",
    "$\\nabla_x (Ax)_i = \\text{ ith row of } A $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_linear_layer(model,target,solver=\"cg\"):\n",
    "    \"\"\"Solve the linear layer problem Mx = b: L2 fitting relu shallow neural networks \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "        relu shallow neural network\n",
    "    target: \n",
    "        a target function \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sol: tensor \n",
    "        the solution of the linear system \n",
    "    \"\"\"\n",
    "    def loss_function_inside(x):\n",
    "        return 0.5*torch.pow(model(x)-target(x),2).to(device)\n",
    "\n",
    "    def rhs_loss_inside(x): \n",
    "        return model(x)*target(x)\n",
    "    #1. Compute loss function using piecewise Gaussian quadrature\n",
    "    node = compute_integration_nodes_relunn(-pi,pi,model)\n",
    "    loss = GQ_piecewise(gw,gx,node,loss_function_inside) #loss_function\n",
    "\n",
    "    #2. Extract the linear system A using torch.autograd \n",
    "    du1 = torch.autograd.grad(outputs=loss, inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "    jac = '' \n",
    "    neuron_number = model.fc1.bias.size(0)\n",
    "    for i in range(neuron_number): \n",
    "        duui = torch.autograd.grad(outputs=du1[0,i], inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "        if i == 0: \n",
    "            jac = duui\n",
    "        else: \n",
    "            jac = torch.cat([jac,duui],dim=0)\n",
    "\n",
    "    #3. Extract the right hand side\n",
    "    loss_helper = GQ_piecewise(gw,gx,node,rhs_loss_inside)\n",
    "    rhs = torch.autograd.grad(outputs=loss_helper, inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "    rhs = rhs.view(-1,1)\n",
    "\n",
    "    #4. Solve the linear system \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach()),np.array(rhs.detach()),tol=1e-10) \n",
    "    elif solver == \"direct\": \n",
    "        sol = np.linalg.inv( np.array(jac.detach()) )@np.array(rhs.detach())\n",
    "    sol = torch.tensor(sol).view(1,-1)\n",
    "    return sol \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02b954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
