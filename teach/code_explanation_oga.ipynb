{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc9864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.9/site-packages/torch/__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1708025540001/work/torch/csrc/tensor/python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "pi = torch.tensor(np.pi,dtype=torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\"  \n",
    "\n",
    "# gx = torch.tensor([-0.9061798459386639927976, -0.5384693101056830910363, 0, 0.5384693101056830910363, \n",
    "#      0.9061798459386639927976]).to(device)\n",
    "# gx = gx.view(1,-1)\n",
    "# gw = torch.tensor([0.2369268850561890875143, 0.4786286704993664680413, 0.5688888888888888888889, 0.4786286704993664680413,\n",
    "#      0.2369268850561890875143]).to(device)\n",
    "# gw = gw.view(-1,1) # Column vector "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55344afc",
   "metadata": {},
   "source": [
    "## Orthogonal greedy algorithm for finite neuron method: $L^2$-fitting\n",
    "\n",
    "\n",
    "**OGA:**\n",
    "\\begin{equation} \n",
    "\t\t\tf_0 = 0, \\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle |, \\quad f_n = P_n(f),  \n",
    "\t\t\\end{equation}\n",
    "\t\twhere $P_n$ is a projection onto $H_n = \\text{span}\\{g_1, g_2,...,g_n \\}$\n",
    "\t\t For 1D $L^2$-fitting for target $f(x)$ with $f(0) = 0$, the ReLU shallow neural network dictionary can be given by \n",
    "\t\t\\begin{equation}\n",
    "\t\t\t\\mathbb{D} = \\{ \\sigma( x + b), b \\in [0,1]  \\}, ~~~ \\sigma(x) = \\max(0,x)\n",
    "\t\t\\end{equation}\n",
    "        \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Theoretically guaranteed convergence rate:** \n",
    "\n",
    "Let the iterates $f_n$ be given by the orthogonal greedy algorithm, where $f\\in \\mathcal{V}_1(\\mathbb{P}_k^d)$, where $\\mathbb{P}_k^d$ is the $\\text{ReLU}^k$ dictionary, $\\sigma_k(\\omega \\cdot x + b)$,  $\\sigma(x) = \\max^k(0,x)$\n",
    "\n",
    "Then we have\n",
    "  \\begin{equation}\n",
    "   \\|f_n - f\\|_{L^2} \\lesssim \\|f\\|_{\\mathcal{V}_1(\\mathbb{P}_k^d)}n^{-\\frac{1}{2} - \\frac{2k+1}{2d}}.\n",
    "  \\end{equation}\n",
    "  \n",
    " **In particular**, for a ReLU shallow neural network in 1D, we have ($k=1, d=1$)\n",
    "   \\begin{equation}\n",
    "   \\|f_n - f\\|_{L^2} \\lesssim \\|f\\|_{\\mathcal{V}_1(\\mathbb{P}_1^1)}n^{-2}.\n",
    "  \\end{equation}\n",
    "  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Two major steps in OGA**:\n",
    "\n",
    "1. argmax \n",
    "2. projection\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**How to implement the two major steps in OGA?**\n",
    "\n",
    "1. Solve the argmax problem $\\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle | $ \n",
    "    - $L^2$ innner product: numerical integration \n",
    "        - Piecewise Gauss quadrature (very accurate numerical quadrature is needed for this part)\n",
    "    - **Method of exhaustion** for 1D for a good initial guess \n",
    "        - $\\mathbb{D}_N = \\{ \\sigma(x + b_i), -b_i = \\frac{i-1}{N_1} \\}_{i = 1}^N$\n",
    "        - $\\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D_N}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle | $ \n",
    "            - store values of $g$ as a row vector, values of $f - f_{n-1}$ column vector\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789d8e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1])\n",
      "torch.Size([100, 50])\n",
      "torch.Size([100, 1])\n",
      "tensor(99)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def relu_dict(x_l,x_r,N):\n",
    "\n",
    "    relu_dict_parameters = torch.zeros((N,2))\n",
    "    relu_dict_parameters[:N,0] = torch.ones(N)[:]\n",
    "    relu_dict_parameters[:N,1] = torch.linspace(x_l,x_r,N+1)[1:] # relu(x+bi)\n",
    "\n",
    "    return relu_dict_parameters\n",
    "def target(x):\n",
    "    return x \n",
    "\n",
    "## 1. ReLU dictionary discretization & Piecewise GQ preparation\n",
    "relu_dict_parameters = relu_dict(-pi,pi,100)\n",
    "integration_intervals = 10 \n",
    "nodes = torch.linspace(-pi,pi,integration_intervals+1).view(-1,1) \n",
    "coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "integration_points = coef1@gx + coef2_expand\n",
    "integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "gw_expand = torch.tile(gw,(integration_intervals,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "func_values = target(integration_points) \n",
    "\n",
    "## 2. Method of exhaustion \n",
    "weight_func_values = func_values*gw_expand*coef1_expand\n",
    "print(weight_func_values.size())\n",
    "basis_values = F.relu(relu_dict_parameters[:,0] *integration_points + relu_dict_parameters[:,1]).T # uses broadcasting  \n",
    "print(basis_values.size())\n",
    "output = torch.abs(torch.matmul(basis_values,weight_func_values)) # each component contains an integral value \n",
    "print(output.size())\n",
    "neuron_index = torch.argmax(output.flatten())\n",
    "\n",
    "print(neuron_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662aad66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ed85b6",
   "metadata": {},
   "source": [
    "2. Orthogonal projection. Given $H_n = \\text{span}\\{g_1, g_2,...,g_n \\}$\n",
    "    - Use w_list, b_list to keep track of the added neurons\n",
    "        - Construct the neural network from the two lists \n",
    "    - Orthogonal projection: Solve the linear system.\n",
    "         - $f_n = P_{H_n} f$  \n",
    "         - $<f_n,g_i> = <f,g_i>$ for all $i= 1,2,...,n$, with $f_n = \\sum_{i=1}^n a_i g_i $\n",
    "         - A linear system in $\\alpha := (a_1, a_2,...,a_n)^T$. $M \\alpha = b$, where $M_{i,j} = <g_j,g_i>$ $b_i = <f,g_i> $\n",
    "         - Equivalently, an $L^2$-fitting: $$\\min_{a_1, a_2,...,a_n} \\frac{1}{2} \\|\\sum_{i=1}^n a_i g_i  - f \\|^2_{L^2}, $$ that is, $$\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2} \\alpha^T M \\alpha - b^T \\alpha $$\n",
    "    - A shortcut: make use of pytorch automatic differentiation to get the matrix M, since we already know how to compute the loss function very accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0eb69",
   "metadata": {},
   "source": [
    "## Assemble the linear system \n",
    "#### Extract the linear system using pytorch auto differentiation and solve the linear system \n",
    "\n",
    "$L(x) = \\frac{1}{2}x^T A x$\n",
    "\n",
    "$\\nabla_x L(x) = A x$\n",
    "\n",
    "$\\nabla_x (Ax)_i = \\text{ ith row of } A $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce450155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_linear_layer(model,target,weights, integration_points,solver=\"direct\"):\n",
    "    \"\"\"Solve the linear layer problem Mx = b: L2 fitting relu shallow neural networks \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "        relu shallow neural network\n",
    "    target: \n",
    "        a target function \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sol: tensor \n",
    "        the solution of the linear system \n",
    "    \"\"\"\n",
    "    def loss_function_inside(x):\n",
    "        return 0.5*torch.pow(model(x)-target(x),2).to(device)\n",
    "\n",
    "    def rhs_loss_inside(x): \n",
    "        return model(x)*target(x)\n",
    "    #1. Compute loss function using piecewise Gaussian quadrature\n",
    "    # node = compute_integration_nodes_relunn(-pi,pi,model)\n",
    "    loss = weights.t()@loss_function_inside(integration_points) #loss_function\n",
    "\n",
    "    #2. Extract the linear system A using torch.autograd \n",
    "    du1 = torch.autograd.grad(outputs=loss, inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "    neuron_number = model.fc1.bias.size(0)\n",
    "    jac = torch.cat([torch.autograd.grad(outputs=du1[0,i], inputs=model.fc2.weight, retain_graph=True,create_graph = False)[0] for i in range(neuron_number)],dim= 0 )\n",
    "\n",
    "    #3. Extract the right hand side\n",
    "    loss_helper = weights.t()@rhs_loss_inside(integration_points) \n",
    "    rhs = torch.autograd.grad(outputs=loss_helper, inputs=model.fc2.weight, retain_graph=True,create_graph = False)[0]\n",
    "    rhs = rhs.view(-1,1)\n",
    "\n",
    "    #4. Solve the linear system \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach()),np.array(rhs.detach()),tol=1e-10) \n",
    "    elif solver == \"direct\": \n",
    "        sol = np.linalg.inv( np.array(jac.detach()) )@np.array(rhs.detach())\n",
    "    sol = torch.tensor(sol).view(1,-1)\n",
    "    return sol \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a507821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_linear_layer_explicit_assemble(model,target,weights, integration_points,solver=\"direct\"):\n",
    "    \"\"\"\n",
    "    calls the following functions (dependency): \n",
    "    1. GQ_piecewise_2D\n",
    "    input: the nn model containing parameter \n",
    "    1. define the loss function  \n",
    "    2. take derivative to extract the linear system A\n",
    "    3. call the cg solver in scipy to solve the linear system \n",
    "    output: sol. solution of Ax = b\n",
    "    \"\"\"\n",
    "    start_time = time.time() \n",
    "    w = model.fc1.weight.data \n",
    "    b = model.fc1.bias.data \n",
    "    basis_value_col = F.relu(integration_points @ w.t()+ b)**(model.k) \n",
    "    weighted_basis_value_col = basis_value_col * weights \n",
    "    jac = weighted_basis_value_col.t() @ basis_value_col \n",
    "     \n",
    "    rhs = weighted_basis_value_col.t() @ (target(integration_points)) \n",
    "#     print(\"assembling the matrix time taken: \", time.time()-start_time) \n",
    "    start_time = time.time()    \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach().cpu()),np.array(rhs.detach().cpu()),tol=1e-12)\n",
    "        sol = torch.tensor(sol).view(1,-1)\n",
    "    elif solver == \"direct\": \n",
    "#         sol = np.linalg.inv( np.array(jac.detach().cpu()) )@np.array(rhs.detach().cpu())\n",
    "        sol = (torch.linalg.solve( jac.detach(), rhs.detach())).view(1,-1)\n",
    "    elif solver == \"ls\":\n",
    "        sol = (torch.linalg.lstsq(jac.detach().cpu(),rhs.detach().cpu(),driver='gelsd').solution).view(1,-1)\n",
    "        # sol = (torch.linalg.lstsq(jac.detach(),rhs.detach()).solution).view(1,-1) # gpu/cpu, driver = 'gels', cannot solve singular\n",
    "#     print(\"solving Ax = b time taken: \", time.time()-start_time)\n",
    "    return sol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a08b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
