{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e27b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "torch.set_default_dtype(torch.torch.float64)\n",
    "pi = torch.tensor(np.pi)\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db2ff3",
   "metadata": {},
   "source": [
    "### 1. Gradient descent for the finite neuron method: $L^2$-fitting\n",
    "\n",
    "\n",
    "1.1 Neural network architecture \n",
    "    $$\\sum_{i=1}^n a_i \\sigma(\\omega_i x + b_i)$$\n",
    " \n",
    "- Access and modify parameters in a neural network \n",
    "\n",
    "1.2 Define a loss function\n",
    "$$\n",
    "\\min_{\\theta} E(\\theta) := \\int_{-\\pi}^{\\pi} \\frac{1}{2} |f(x) - \\sum_{i=1}^{n} a_i \\sigma( x + b_i) |^2 dx, \\quad \\theta = \\{a_i,b_i\\}_{i=1}^n\n",
    "$$\n",
    "        \n",
    "1.3 Create an optimizer in pytorch and tune it parameters \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc1b0f",
   "metadata": {},
   "source": [
    "A simple one-hidden layer shallow neural network: \n",
    "$$\\sum_{i=1}^n a_i \\sigma(\\omega_i x + b_i)$$\n",
    "$$ x \\rightarrow W_1x+b \\rightarrow \\sigma(W_1x +b) \\rightarrow W_2(\\sigma(W_1x))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6a7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 Neural network architecture \n",
    "class model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1) # W1, b\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False) # W2\n",
    "\n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x)))\n",
    "        return u1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51071cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6466],\n",
      "        [ 0.4610],\n",
      "        [ 0.1265],\n",
      "        [ 0.3587]])\n",
      "True\n",
      "tensor([0.1507, 0.1685, 0.1006, 0.8667])\n",
      "True\n",
      "tensor([[ 0.3107, -0.4210,  0.4471,  0.4535]])\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 1.2 Access and modify parameters in a neural network \n",
    "\"\"\"\n",
    "Instantiate a class object my_model\n",
    "Access the parameters\n",
    "Mutate(modify) the parameters\n",
    "\"\"\"\n",
    "neuron_num = 4\n",
    "my_model = model(1,neuron_num,1)\n",
    "\n",
    "\"\"\"\n",
    "1. Access parameters using my_model.parameters()\n",
    "2. torch parameters contain two attributes: data and requires_grad\n",
    "\"\"\"\n",
    "for parameter in my_model.parameters():\n",
    "    print(parameter.data)\n",
    "    print(parameter.requires_grad)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1ce9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.6466],\n",
      "        [ 0.4610],\n",
      "        [ 0.1265],\n",
      "        [ 0.3587]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1507, 0.1685, 0.1006, 0.8667], requires_grad=True)\n",
      "tensor([[-0.6466],\n",
      "        [ 0.4610],\n",
      "        [ 0.1265],\n",
      "        [ 0.3587]])\n",
      "True\n",
      "\n",
      "+++++++++++++\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Directly access the class attributes: fc1, fc2\n",
    "2. Access the paramters as attributes of fc1, fc2\n",
    "\"\"\"\n",
    "print(my_model.fc1.weight)\n",
    "print(my_model.fc1.bias)\n",
    "print(my_model.fc1.weight.data)\n",
    "print(my_model.fc1.weight.requires_grad)\n",
    "print()\n",
    "print(\"+++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f941a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1143],\n",
      "        [-0.8234],\n",
      "        [-0.9260],\n",
      "        [-0.9157]])\n",
      "True\n",
      "tensor([-0.2767,  0.7563, -0.9077,  0.3532])\n",
      "True\n",
      "tensor([[ 0.0165, -0.2589, -0.3392, -0.0975]])\n",
      "True\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.1143],\n",
      "        [-0.8234],\n",
      "        [-0.9260],\n",
      "        [-0.9157]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2767,  0.7563, -0.9077,  0.3532], requires_grad=True)\n",
      "tensor([[ 0.1143],\n",
      "        [-0.8234],\n",
      "        [-0.9260],\n",
      "        [-0.9157]])\n",
      "True\n",
      "\n",
      "+++++++++++++\n",
      "Parameter containing:\n",
      "tensor([[ 0.1143],\n",
      "        [-0.8234],\n",
      "        [-0.9260],\n",
      "        [-0.9157]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2767,  0.7563, -0.9077,  0.3532], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1143],\n",
      "        [-0.8234],\n",
      "        [-0.9260],\n",
      "        [-0.9157]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2767,  0.7563, -0.9077,  0.3532], requires_grad=True)\n",
      "=========\n",
      "tensor([[ 0.1143],\n",
      "        [-0.8234],\n",
      "        [-0.9260],\n",
      "        [-0.9157]])\n",
      "Parameter containing:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## 1.2 Access and modify parameters in a neural network \n",
    "\"\"\"\n",
    "Instantiate a class object my_model\n",
    "Access the parameters\n",
    "Mutate(modify) the parameters\n",
    "\"\"\"\n",
    "neuron_num = 4\n",
    "my_model = model(1,neuron_num,1)\n",
    "\n",
    "\"\"\"\n",
    "1. Access parameters using my_model.parameters()\n",
    "2. torch parameters contain two attributes: data and requires_grad\n",
    "\"\"\"\n",
    "for parameter in my_model.parameters():\n",
    "    print(parameter.data)\n",
    "    print(parameter.requires_grad)\n",
    "print()\n",
    "\n",
    "\"\"\"\n",
    "1. Directly access the class attributes: fc1, fc2\n",
    "2. Access the paramters as attributes of fc1, fc2\n",
    "\"\"\"\n",
    "print(my_model.fc1.weight)\n",
    "print(my_model.fc1.bias)\n",
    "print(my_model.fc1.weight.data)\n",
    "print(my_model.fc1.weight.requires_grad)\n",
    "print()\n",
    "print(\"+++++++++++++\")\n",
    "for param in my_model.fc1.parameters():\n",
    "    print(param)\n",
    "    param.requires_grad = False \n",
    "    param.requires_grad_()\n",
    "print(my_model.fc1.weight)\n",
    "print(my_model.fc1.bias)\n",
    "\n",
    "\"\"\"\n",
    "Modify the parameter:\n",
    "Shape matches; modify data and requires_grad separately\n",
    "\"\"\"\n",
    "print(\"=========\")\n",
    "w_i = torch.full((1,neuron_num),1.0).view(neuron_num,1)\n",
    "print(my_model.fc1.weight.data)\n",
    "my_model.fc1.weight.data = w_i # w_i is now the data\n",
    "my_model.fc1.weight.requires_grad = False \n",
    "print(my_model.fc1.weight)\n",
    "print(my_model.fc1.weight.data)\n",
    "print(my_model.fc1.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce2d55",
   "metadata": {},
   "source": [
    "#### It is easy to compute the nodal points of the relu shallow neural network. \n",
    "$$\\sum_{i=1}^n a_i \\sigma(\\omega_i x + b_i)$$\n",
    "\n",
    "The nodal points are $\\{-\\frac{b_i}{\\omega_i} \\}_{i=1}^n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce128f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_integration_nodes_relunn(a,b,model): \n",
    "    weight = model.fc1.weight.detach().squeeze()\n",
    "    bias = model.fc1.bias.detach() \n",
    "    neuron_number = bias.size(0)\n",
    "    node = torch.empty(neuron_number + 2).to(device)\n",
    "    node[-1] = a\n",
    "    node[-2] = b \n",
    "    node[0:neuron_number] = - bias / weight \n",
    "    node = node[(node <= b)] \n",
    "    node = node[(node >= a)]\n",
    "    node = node.view(-1,1)\n",
    "    if neuron_number < 100: \n",
    "        refined_node = torch.linspace(a,b,100).view(-1,1)\n",
    "        node = torch.cat([node,refined_node])\n",
    "    node = node.unique()\n",
    "    node, indices = torch.sort(node)\n",
    "    node = node.view(-1,1)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe484c",
   "metadata": {},
   "source": [
    "#### 1.3 Define a loss function using piecewise Gauss quadrature\n",
    "\n",
    "1.3 Define a loss function\n",
    "$$\n",
    "\t\\min_{\\theta} E(\\theta) := \\int_{-\\pi}^{\\pi} \\frac{1}{2} |f(x) - \\sum_{i=1}^{n} a_i \\sigma( x + b_i) |^2 dx, \\text{ where } \\theta = \\{a_i,b_i\\}_{i=1}^n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a0ac6",
   "metadata": {},
   "source": [
    "#### Standard p-point Gaussian quadrature rule on $[-1,1]$\n",
    "\\begin{equation}\n",
    "    \\int_{-1}^{1} f(x) dx \\approx \\sum_{i = 1}^p w_i f(g_i).\n",
    "\\end{equation}\n",
    "\n",
    "e.g. 5 point Gaussian quadrature: \n",
    "\n",
    "$g_i, w_i$ (left, right):\n",
    "\\begin{aligned}\n",
    "& 0, ~~~~ \\frac{128}{225}\\\\\n",
    "& \\pm \\frac{1}{3} \\sqrt{5-2 \\sqrt{\\frac{10}{7}}}, ~ \\frac{322+13 \\sqrt{70}}{900} \\\\\n",
    "& \\pm \\frac{1}{3} \\sqrt{5+2 \\sqrt{\\frac{10}{7}}}, ~\\frac{322-13 \\sqrt{70}}{900}\n",
    "\\end{aligned}\n",
    "\n",
    "On an arbitrary interval $[x_i, x_{i+1}]$: \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "        \\int_{x_i}^{x_{i+1}} f(x) dx & = \\frac{x_{i+1} - x_{i}}{2 } \\int_{-1}^1 f(\\frac{x_{i+1} - x_i}{2} \\xi + \\frac{x_{i+1} + x_i}{2}) d\\xi \\\\\n",
    "        & \\approx \\frac{x_{i+1} - x_{i}}{2 } \\sum_{j =1}^p w_j f(\\frac{x_{i+1} - x_i}{2}  g_j + \\frac{x_{i+1} + x_i}{2}) \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "**Method**\n",
    "- Divide the domain (interval) into several subdomains (subintervals). $[x_0,x_1], [x_1,x_2],...,[x_{N-1}, x_{N}]$\n",
    "- Compute the quadrature value in each subdomain.\n",
    "- Sum the quadrature values in each subdomain to get the quadrature value over the whole domain.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ee8e8",
   "metadata": {},
   "source": [
    "**Vectorization**: \n",
    "\n",
    "$$\\int_{x_i}^{x_{i+1}} f(x) dx \n",
    "\\frac{x_{i+1} - x_{i}}{2 } \\approx \\frac{x_{i+1} - x_{i}}{2 } \\sum_{j =1}^p w_j f(\\frac{x_{i+1} - x_i}{2}  g_j + \\frac{x_{i+1} + x_i}{2}),  i= 0,...,N-1 $$\n",
    "\n",
    "Sum over $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e9f7432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.2832]])\n"
     ]
    }
   ],
   "source": [
    "### 5 point Gauss Quadrature rule\n",
    "def integrand(x):\n",
    "    return torch.sin(10*x)+1 \n",
    "\n",
    "## the following quadrature weights and nodes are provied in numpy \n",
    "# gx = torch.tensor([-0.9061798459386639927976, -0.5384693101056830910363, 0, 0.5384693101056830910363, \n",
    "#      0.9061798459386639927976]).to(device)\n",
    "# gx = gx.view(1,-1)\n",
    "# gw = torch.tensor([0.2369268850561890875143, 0.4786286704993664680413, 0.5688888888888888888889, 0.4786286704993664680413,\n",
    "#      0.2369268850561890875143]).to(device)\n",
    "# gw = gw.view(-1,1) \n",
    "order = 5 \n",
    "x,w = np.polynomial.legendre.leggauss(order)\n",
    "gx = torch.tensor(x).to(device)\n",
    "gx = gx.view(1,-1) # row vector \n",
    "gw = torch.tensor(w).to(device)    \n",
    "gw = gw.view(-1,1) # Column vector \n",
    "\n",
    "num_points = 12 # subintervals\n",
    "nodes = torch.linspace(-pi,pi,num_points+1).view(-1,1) \n",
    "coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "integration_points = coef1@gx + coef2_expand\n",
    "integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "gw_expand = torch.tile(gw,(num_points,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "# Modify coef1 to be compatible with func_values\n",
    "coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "\n",
    "func_values = integrand(integration_points)\n",
    "integral_value = torch.matmul(func_values.T,gw_expand*coef1_expand) #integral_value = torch.sum(func_values*gw_expand*coef1_expand)\n",
    "print(integral_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46093c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2510826d",
   "metadata": {},
   "source": [
    "##### 1.4 Choices of optimizers \n",
    "\n",
    "Suppose we have a loss function $E(\n",
    "\\theta)$,\n",
    "\n",
    "- Gradient descent: \n",
    "$$\\theta^{k+1} = \\theta^k - \\eta_k \\nabla_\\theta E(\\theta^k)$$\n",
    "\n",
    "- Gradient descent with momentum \n",
    "$$\\theta^{k+1} = \\theta^k - \\eta \\bigg( \\mu (\\theta^k - \\theta^{k-1})  + (1 - \\mu)\\nabla_\\theta E(\\theta^k) \\bigg) $$\n",
    "\n",
    "For this simple example, we can compute the gradient by hand. \n",
    "\n",
    "$$\n",
    "\\nabla_a E(\\theta) = \\int_{-\\pi}^{\\pi} \\big( \\sum_{i=1}^{n} a_i \\sigma( x + b_i) -  f(x)\\big) \\begin{pmatrix} \\\\\n",
    "\\sigma( x + b_1)\\\\\n",
    "\\sigma( x + b_2)\\\\\n",
    "\\vdots\\\\\n",
    "\\sigma( x + b_n)\n",
    "\\end{pmatrix} dx\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b E(\\theta) = \\int_{-\\pi}^{\\pi} \\big( \\sum_{i=1}^{n} a_i \\sigma( x + b_i) -  f(x)\\big) \\begin{pmatrix} \\\\\n",
    "a_1\\sigma'( x + b_1)\\\\\n",
    "a_2\\sigma'( x + b_2)\\\\\n",
    "\\vdots\\\\\n",
    "a_n\\sigma'( x + b_n)\n",
    "\\end{pmatrix} dx\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02941d9",
   "metadata": {},
   "source": [
    "##### Create an optimizer in pytorch and tune it parameters \n",
    "- import torch.optim as optim\n",
    "- link: https://pytorch.org/docs/stable/optim.html, https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "- Schedule learning rate\n",
    "\n",
    "- Basic syntax:\n",
    "        \n",
    "        optimizer = optim.SGD(my_model.parameters(), lr=0.02) # Create an optimizer\n",
    "        \n",
    "        optimizer.zero_grad() # clear the gradient wrt parameters \n",
    "    \n",
    "        output = model(input) \n",
    "    \n",
    "        loss = loss_fn(output, target) # define the loss \n",
    "    \n",
    "        loss.backward() # compute the gradients \n",
    "    \n",
    "        optimizer.step() # gradient step: $\\theta^{k+1} = \\theta^k - \\eta \\nabla_\\theta E(\\theta^k)$\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.4 Create an optimizer in pytorch and tune it parameters \n",
    "\n",
    "# Define optimizer \n",
    "optimizer = optim.SGD(my_model.parameters(), lr=0.02)\n",
    "# optimizer = optim.Adam(my_model.parameters(), lr=0.02)\n",
    "\n",
    "# Learning rate schedule\n",
    "lr = 0.02\n",
    "for epoch in range(20):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr * (0.98 ** ((epoch + 1) // 1000)) # Learning rate schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abf9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4fe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
