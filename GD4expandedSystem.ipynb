{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a896a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "torch.set_default_tensor_type(torch.DoubleTensor) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e97655",
   "metadata": {},
   "source": [
    "Consider: $A_\\epsilon u=g$, ($A_\\epsilon = A_0+\\epsilon I$)\n",
    "$$\n",
    "A_0 =\n",
    "\\left(\n",
    "\\begin{array}{rrr}\n",
    "1  &  -1   &  0\\\\\n",
    "-1 &   2   &  -1\\\\\n",
    "0  &  -1   &  1\n",
    "\\end{array}\n",
    "\\right),\\quad\n",
    "g=\n",
    "\\left(\n",
    "\\begin{array}{r}\n",
    "-1 \\\\\n",
    "-1 \\\\\n",
    "2  \\\\\n",
    "\\end{array}\n",
    "\\right)\\in R(A_0), \\quad\n",
    "p=\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\in N(A_0).\n",
    "$$\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4b646",
   "metadata": {},
   "source": [
    "For $f(u) = \\frac{1}{2}u^T A u -g^T u$\n",
    "\n",
    "Gradient descent method: \n",
    "\n",
    "$$\n",
    "u^{k+1} = u^{k} - \\eta \\nabla f(u^{k})\n",
    " =u^{k} - \\eta (Au^{k}-g)\n",
    "$$\n",
    "\n",
    "Scaled gradient descent\n",
    "$$\n",
    "u^{k+1} =u^{k} - \\eta [{\\rm diag}(A)]^{-1}(Au^{k}-g)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72452a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GD for 3by3 system \n",
    "print(\"Plain GD: number of iterations needed for 3 by 3 system\")\n",
    "for eps in [0.1,0.01,0.001,1e-4,1e-5,1e-9, 0.]: \n",
    "    A3 = torch.tensor([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]])\n",
    "    x = torch.zeros(3)\n",
    "    x = x.view(3,1)\n",
    "    b = torch.tensor([[-1.],[-1.],[2.]]) # must be in kernel is eps = 0\n",
    "    x.data = torch.tensor([[1.0],[2.],[3.0]])\n",
    "    tol = 1e-8 # tolerance for residual norm \n",
    "    residual_norm = torch.norm(torch.matmul(A3,x) -b,2)\n",
    "    iters = 0 \n",
    "    while residual_norm > tol: \n",
    "        gd = torch.matmul(A3,x) - b \n",
    "        x = x - 0.5*gd \n",
    "        residual_norm = torch.norm(gd,2)\n",
    "        iters += 1 \n",
    "        if iters > 1000000: \n",
    "            break\n",
    "        assert torch.isnan(residual_norm)!=True, \"norm is nan, reset learning rate\" #somehow nan>tol returns false\n",
    "    if iters > 1000000: \n",
    "        print(\"eps = \"+str(eps)+\": over 1,000,000\")\n",
    "    else:\n",
    "        print(\"eps = \"+str(eps)+\": \", iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027db488",
   "metadata": {},
   "source": [
    "Expanded system:\n",
    "    \n",
    "Write $u\\in \\mathbb{R}^3=u_1e_1+u_2e_2+u_3e_3$ as\n",
    "$$\n",
    "u=\\tilde u_1 e_1+\\tilde u_2e_2+\\tilde u_3e_3+\\tilde\n",
    "    u_4 p =P\\tilde u,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "P=\\begin{pmatrix}\n",
    "    1 & 0 & 0 & 1\\\\\n",
    "    0 & 1 & 0 & 1\\\\\n",
    "    0 & 0 & 1 & 1\n",
    "\\end{pmatrix}, \\quad p=\n",
    "\\begin{pmatrix}\n",
    "    1 \\\\ 1 \\\\ 1\n",
    "\\end{pmatrix}\n",
    "\\in {\\rm ker}(A_0). \n",
    "$$\n",
    "\n",
    "The equation $A_{\\epsilon}u=g$ becomes\n",
    "$$\n",
    "A_{\\epsilon}P\\tilde u=g \\Longleftrightarrow\n",
    "(P^TA_{\\epsilon}P)\\tilde u=P^Tg,\n",
    "$$\n",
    "\n",
    "This leads to a semi-definite system:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    1+\\epsilon  &  -1   &  0&\\epsilon\\\\\n",
    "    -1 &   2+\\epsilon   &  -1&\\epsilon\\\\\n",
    "    0  &  -1   &  1+\\epsilon&\\epsilon\\\\\n",
    "    \\epsilon&\\epsilon&\\epsilon&3\\epsilon\n",
    "\\end{pmatrix}\n",
    "\\tilde u=\n",
    "\\begin{pmatrix}\n",
    "      -1 \\\\\n",
    "    -1 \\\\\n",
    "    2  \\\\\n",
    "    0\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ed6f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD: 4 by 4 system\n",
      "eps = 0.1:  71\n",
      "eps = 0.01:  714\n",
      "eps = 0.001:  6264\n",
      "eps = 0.0001:  48098\n",
      "eps = 1e-05:  100001\n",
      "eps = 1e-09:  28\n",
      "eps = 0.0:  29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## GD for 4by4 system, GD\n",
    "print(\"GD: 4 by 4 system\")\n",
    "P = torch.tensor([[1.,0.,0.,1.],[0.,1.,0.,1.],[0.,0.,1.,1.]])\n",
    "for eps in [0.1,0.01,0.001,1e-4,1e-5,1e-9,0.]: \n",
    "    A3 = torch.tensor([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]])\n",
    "    A4 = torch.tensor([[1+eps,-1.0,0,eps],[-1,2+eps,-1,eps],[0,-1,1+eps,eps],[eps,eps,eps,3*eps]])\n",
    "    x = torch.rand(4)\n",
    "    x = x.view(4,1)\n",
    "    b = torch.tensor([[-1.],[-1.],[2.],[0.]]) #\n",
    "    tol = 1e-8\n",
    "    residual_norm = torch.norm(A3@(P@x)-P@b,2)\n",
    "    iters = 0 \n",
    "    while residual_norm > tol: \n",
    "        gd = torch.matmul(A4,x) - b \n",
    "        x.data = x.data - 0.5*gd \n",
    "        residual_norm = torch.norm(A3@(P@x)-P@b,2)\n",
    "        iters += 1 \n",
    "        if iters > 100000: \n",
    "            break\n",
    "    assert torch.isnan(residual_norm)!=True, \"norm is nan, reset learning rate\"\n",
    "    print(\"eps = \"+str(eps)+\": \", iters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f02f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled GD: 4 by 4 system\n",
      "eps = 0.1:  17\n",
      "eps = 0.01:  21\n",
      "eps = 0.001:  21\n",
      "eps = 0.0001:  21\n",
      "eps = 1e-05:  21\n",
      "eps = 1e-09:  21\n"
     ]
    }
   ],
   "source": [
    "#GD for 4by4 system, modified Jacobi preconditioner\n",
    "print(\"Scaled GD: 4 by 4 system\")\n",
    "P = torch.tensor([[1.,0.,0.,1.],[0.,1.,0.,1.],[0.,0.,1.,1.]])\n",
    "for eps in [0.1,0.01,0.001,1e-4,1e-5,1e-9]: \n",
    "    A3 = torch.tensor([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]])\n",
    "    A4 = torch.tensor([[1+eps,-1.0,0,eps],[-1,2+eps,-1,eps],[0,-1,1+eps,eps],[eps,eps,eps,3*eps]])\n",
    "    D = torch.diag(torch.diag(A4))\n",
    "    x = torch.rand(4)\n",
    "    x = x.view(4,1)\n",
    "    b = torch.tensor([[-1.],[-1.],[2.],[0.]]) \n",
    "    tol = 1e-8\n",
    "    residual_norm = torch.norm(A3@(P@x)-P@b,2)\n",
    "    iters = 0 \n",
    "    while residual_norm > tol: \n",
    "        gd = torch.matmul(A4,x) - b \n",
    "        x.data = x.data - 0.7*torch.matmul(torch.linalg.inv(D),gd)\n",
    "        residual_norm = torch.norm(A3@(P@x)-P@b,2)\n",
    "        iters += 1 \n",
    "        if iters > 100000: \n",
    "            break\n",
    "    assert torch.isnan(residual_norm)!=True, \"norm is nan, reset learning rate\"\n",
    "    print(\"eps = \"+str(eps)+\": \", iters)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dd60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
