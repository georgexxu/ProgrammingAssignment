{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a896a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# from jax import config\n",
    "# config.update(\"jax_enable_x64\", True) # Use double precision in JAX\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "torch.set_default_dtype(torch.float64) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e97655",
   "metadata": {},
   "source": [
    "Consider: $A_\\epsilon u=g$, ($A_\\epsilon = A_0+\\epsilon I$)\n",
    "$$\n",
    "A_0 =\n",
    "\\left(\n",
    "\\begin{array}{rrr}\n",
    "1  &  -1   &  0\\\\\n",
    "-1 &   2   &  -1\\\\\n",
    "0  &  -1   &  1\n",
    "\\end{array}\n",
    "\\right),\\quad\n",
    "g=\n",
    "\\left(\n",
    "\\begin{array}{r}\n",
    "-1 \\\\\n",
    "-1 \\\\\n",
    "2  \\\\\n",
    "\\end{array}\n",
    "\\right)\\in R(A_0), \\quad\n",
    "p=\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\in N(A_0).\n",
    "$$\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e4b646",
   "metadata": {},
   "source": [
    "For $f(x) = \\frac{1}{2}x^T A_\\epsilon x -g^T x$\n",
    "\n",
    "Gradient descent method (Richardson's method): \n",
    "\n",
    "$$\n",
    "x^{k+1} = x^{k} - \\eta \\nabla f(x^{k})\n",
    " =x^{k} - \\eta (A_\\epsilon x^{k}-g)\n",
    "$$\n",
    "\n",
    "Scaled gradient descent (Damped Jacobi)\n",
    "$$\n",
    "x^{k+1} =x^{k} - \\eta [{\\rm diag}(A_\\epsilon)]^{-1}(A_\\epsilon x^{k}-g)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6658186",
   "metadata": {},
   "source": [
    "**Experiments**\n",
    "\n",
    "- start with a random initial guess $x_0$\n",
    "- start with $x_0 = \\bf{0}$ \n",
    "- start with $x_0 = [1, 0 , -1]^T$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd445ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72452a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain GD: number of iterations needed for 3 by 3 system\n",
      "eps = 0.1:  321\n",
      "eps = 0.01:  2551\n",
      "eps = 0.001:  22520\n",
      "eps = 0.0001:  187769\n",
      "eps = 1e-05: over 1,000,000\n",
      "eps = 1e-09:  29\n",
      "eps = 0.0:  30\n",
      "CPU times: user 32.7 s, sys: 146 ms, total: 32.8 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "## GD for 3by3 system \n",
    "\n",
    "print(\"Plain GD: number of iterations needed for 3 by 3 system\")\n",
    "for eps in [0.1,0.01,1e-3,1e-4,1e-5,1e-9, 0.]: \n",
    "    A3 = torch.tensor([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]])\n",
    "#     x = torch.zeros(3)\n",
    "    x = torch.rand(3)\n",
    "    x = x.view(3,1)\n",
    "    b = torch.tensor([[-1.],[-1.],[2.]]) # must be in kernel if eps = 0\n",
    "#     x.data = torch.tensor([[1.0],[0.],[-1.0]])\n",
    "    tol = 1e-8 # tolerance for residual norm \n",
    "    residual_norm = torch.norm(torch.matmul(A3,x) -b,2)\n",
    "    iters = 0\n",
    "    while residual_norm > tol: \n",
    "        gd = torch.matmul(A3,x) - b \n",
    "        x = x - 0.5*gd \n",
    "\n",
    "        residual_norm = torch.norm(gd,2)\n",
    "        iters += 1 \n",
    "        if iters > 1000000: \n",
    "            break\n",
    "        assert torch.isnan(residual_norm)!=True, \"norm is nan, reset learning rate\" #somehow nan>tol returns false\n",
    "    if iters > 1000000: \n",
    "        print(\"eps = \"+str(eps)+\": over 1,000,000\")\n",
    "    else:\n",
    "        print(\"eps = \"+str(eps)+\": \", iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2a20bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain GD: number of iterations needed for 3 by 3 system\n",
      "eps = 0.1:  402\n",
      "eps = 0.01:  3202\n",
      "eps = 0.001:  25711\n",
      "eps = 0.0001:  195575\n",
      "eps = 1e-05: over 1,000,000\n",
      "eps = 1e-09:  30\n",
      "eps = 0.0:  30\n",
      "CPU times: user 36.3 s, sys: 177 ms, total: 36.4 s\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "## Scaled GD for 3by3 system \n",
    "\n",
    "print(\"Plain GD: number of iterations needed for 3 by 3 system\")\n",
    "for eps in [0.1,0.01,1e-3,1e-4,1e-5,1e-9, 0.]: \n",
    "    A3 = torch.tensor([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]])\n",
    "    D_inv = torch.tensor([[1/(1+eps),0,0],[0,1/(2+eps),0],[0,0,1/(1+eps)]])\n",
    "    x = torch.zeros(3)\n",
    "    x = x.view(3,1)\n",
    "    b = torch.tensor([[-1.],[-1.],[2.]]) # must be in kernel if eps = 0\n",
    "    x.data = torch.tensor([[1.0],[0.],[-1.0]])\n",
    "    tol = 1e-8 # tolerance for residual norm \n",
    "    residual_norm = torch.norm(torch.matmul(A3,x) -b,2)\n",
    "    iters = 0\n",
    "    while residual_norm > tol: \n",
    "        gd = torch.matmul(A3,x) - b \n",
    "        x = x - 0.5*D_inv @ gd \n",
    "\n",
    "        residual_norm = torch.norm(gd,2)\n",
    "        iters += 1 \n",
    "        if iters > 1000000: \n",
    "            break\n",
    "        assert torch.isnan(residual_norm)!=True, \"norm is nan, reset learning rate\" #somehow nan>tol returns false\n",
    "    if iters > 1000000: \n",
    "        print(\"eps = \"+str(eps)+\": over 1,000,000\")\n",
    "    else:\n",
    "        print(\"eps = \"+str(eps)+\": \", iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027db488",
   "metadata": {},
   "source": [
    "**Expanded system**:\n",
    "    \n",
    "Write $u\\in \\mathbb{R}^3=u_1e_1+u_2e_2+u_3e_3$ as\n",
    "$$\n",
    "u=\\tilde u_1 e_1+\\tilde u_2e_2+\\tilde u_3e_3+\\tilde\n",
    "    u_4 p =P\\tilde u,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "P=\\begin{pmatrix}\n",
    "    1 & 0 & 0 & 1\\\\\n",
    "    0 & 1 & 0 & 1\\\\\n",
    "    0 & 0 & 1 & 1\n",
    "\\end{pmatrix}, \\quad p=\n",
    "\\begin{pmatrix}\n",
    "    1 \\\\ 1 \\\\ 1\n",
    "\\end{pmatrix}\n",
    "\\in {\\rm ker}(A_0). \n",
    "$$\n",
    "\n",
    "The equation $A_{\\epsilon}u=g$ becomes\n",
    "$$\n",
    "A_{\\epsilon}P\\tilde u=g \\Longleftrightarrow\n",
    "(P^TA_{\\epsilon}P)\\tilde u=P^Tg,\n",
    "$$\n",
    "\n",
    "This leads to a semi-definite system:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    1+\\epsilon  &  -1   &  0&\\epsilon\\\\\n",
    "    -1 &   2+\\epsilon   &  -1&\\epsilon\\\\\n",
    "    0  &  -1   &  1+\\epsilon&\\epsilon\\\\\n",
    "    \\epsilon&\\epsilon&\\epsilon&3\\epsilon\n",
    "\\end{pmatrix}\n",
    "\\tilde u=\n",
    "\\begin{pmatrix}\n",
    "      -1 \\\\\n",
    "    -1 \\\\\n",
    "    2  \\\\\n",
    "    0\\\\\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ed6f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD: 4 by 4 system\n",
      "eps = 0.1:  76\n",
      "eps = 0.01:  699\n",
      "eps = 0.001:  6126\n",
      "eps = 0.0001:  48390\n",
      "eps = 1e-05:  100001\n",
      "eps = 1e-09:  28\n",
      "eps = 0.0:  29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## GD for 4by4 system, GD\n",
    "print(\"GD: 4 by 4 system\")\n",
    "P = torch.tensor([[1.,0.,0.,1.],[0.,1.,0.,1.],[0.,0.,1.,1.]])\n",
    "for eps in [0.1,0.01,0.001,1e-4,1e-5,1e-9,0.]: \n",
    "    A3 = torch.tensor([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]])\n",
    "    A4 = torch.tensor([[1+eps,-1.0,0,eps],[-1,2+eps,-1,eps],[0,-1,1+eps,eps],[eps,eps,eps,3*eps]])\n",
    "    x = torch.rand(4)\n",
    "    x = x.view(4,1)\n",
    "    b = torch.tensor([[-1.],[-1.],[2.],[0.]]) #\n",
    "    tol = 1e-8\n",
    "    residual_norm = torch.norm(A3@(P@x)-P@b,2)\n",
    "    iters = 0 \n",
    "    while residual_norm > tol: \n",
    "        gd = torch.matmul(A4,x) - b \n",
    "        x.data = x.data - 0.5*gd \n",
    "        residual_norm = torch.norm(A3@(P@x)-P@b,2)\n",
    "        iters += 1 \n",
    "        if iters > 100000: \n",
    "            break\n",
    "    assert torch.isnan(residual_norm)!=True, \"norm is nan, reset learning rate\"\n",
    "    print(\"eps = \"+str(eps)+\": \", iters)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f02f394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled GD: 4 by 4 system\n",
      "eps = 0.1:  17\n",
      "eps = 0.01:  21\n",
      "eps = 0.001:  21\n",
      "eps = 0.0001:  21\n",
      "eps = 1e-05:  20\n",
      "eps = 1e-09:  21\n"
     ]
    }
   ],
   "source": [
    "#GD for 4by4 system, modified Jacobi preconditioner\n",
    "print(\"Scaled GD: 4 by 4 system\")\n",
    "P = torch.tensor([[1.,0.,0.,1.],[0.,1.,0.,1.],[0.,0.,1.,1.]])\n",
    "for eps in [0.1,0.01,0.001,1e-4,1e-5,1e-9]: \n",
    "    A3 = torch.tensor([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]])\n",
    "    A4 = torch.tensor([[1+eps,-1.0,0,eps],[-1,2+eps,-1,eps],[0,-1,1+eps,eps],[eps,eps,eps,3*eps]])\n",
    "    D = torch.diag(torch.diag(A4))\n",
    "    x = torch.rand(4)\n",
    "    x = x.view(4,1)\n",
    "    b = torch.tensor([[-1.],[-1.],[2.],[0.]]) \n",
    "    tol = 1e-8\n",
    "    residual_norm = torch.norm(A3@(P@x)-P@b,2)\n",
    "    iters = 0 \n",
    "    while residual_norm > tol: \n",
    "        gd = torch.matmul(A4,x) - b \n",
    "        x.data = x.data - 0.7*torch.matmul(torch.linalg.inv(D),gd)\n",
    "        residual_norm = torch.norm(A3@(P@x)-P@b,2)\n",
    "        iters += 1 \n",
    "        if iters > 100000: \n",
    "            break\n",
    "    assert torch.isnan(residual_norm)!=True, \"norm is nan, reset learning rate\"\n",
    "    print(\"eps = \"+str(eps)+\": \", iters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0dd60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac8121d3",
   "metadata": {},
   "source": [
    "## Playground for JAX\n",
    "\n",
    "This part was added after the CEMRACRS 2023. On day 4, we were introduced to using JAX for improving code efficiency in python (Martin Guerra from UW Madison. So I rewrote my code in JAX for a test. See git repo https://github.com/maguerrap). \n",
    "\n",
    "- Automatic differentiation with JAX.\n",
    "- JIT to improve efficiency \n",
    "\n",
    "For the 3 by 3 system, we run GD for 1000 steps. \n",
    "\n",
    "1. Use the function gd_update  (6.28s)\n",
    "2. Use the function gd_update_jit (0.10s)\n",
    "3. Explicitly computed gradient (0.056s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1eeb7c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps = 1e-05: over 1000 iterations\n",
      "CPU times: user 5.95 s, sys: 18.4 ms, total: 5.97 s\n",
      "Wall time: 5.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "## JAX version GD. For an equivalent problem, we use gradient descent for the quadratic loss \n",
    "def loss_3by3(x):\n",
    "    \"\"\"\n",
    "    x: column vector \n",
    "    \"\"\"\n",
    "    return jnp.sum(0.5*x.T@A3@x - x.T@b )\n",
    "\n",
    "def gd_update(x,lr):\n",
    "    return x - lr*jax.grad(loss_3by3)(x)\n",
    "def residual_norm(A3,x,b):\n",
    "    return jnp.linalg.norm(A3@x - b ,2)\n",
    "eps = 1e-5  \n",
    "A3 = jnp.asarray([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]]) \n",
    "b = jnp.asarray([-1.,-1.,2.]).reshape(3,1)\n",
    "x = jnp.asarray([1.,2.,3.]).reshape(3,1)\n",
    "tol = 1e-8\n",
    "lr = 0.5 \n",
    "\n",
    "num_iters = 1000\n",
    "for i in range(num_iters):\n",
    "    x = gd_update(x,lr)\n",
    "    if residual_norm(A3,x,b) < tol: \n",
    "        print(\"eps = \"+str(eps)+\": \", i)\n",
    "        break \n",
    "    elif i == num_iters-1:\n",
    "        print(\"eps = \"+str(eps)+\": over {} iterations\".format(num_iters))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aff4c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps = 1e-05: over 1000 iterations\n",
      "CPU times: user 129 ms, sys: 6 ms, total: 135 ms\n",
      "Wall time: 134 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "num_iters = 1000\n",
    "gd_update_jit = jax.jit(gd_update, static_argnames = 'lr')\n",
    "residual_norm_jit = jax.jit(residual_norm)\n",
    "\n",
    "for i in range(num_iters):\n",
    "    x = gd_update_jit(x,lr)\n",
    "    if residual_norm_jit(A3,x,b) < tol: \n",
    "        print(\"eps = \"+str(eps)+\": \", i)\n",
    "        break \n",
    "    elif i == num_iters-1:\n",
    "        print(\"eps = \"+str(eps)+\": over {} iterations\".format(num_iters))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e7cef0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain GD: number of iterations needed for 3 by 3 system\n",
      "eps = 1e-05: over 1000\n",
      "CPU times: user 35.6 ms, sys: 2.25 ms, total: 37.8 ms\n",
      "Wall time: 36.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "## GD for 3by3 system \n",
    "\n",
    "print(\"Plain GD: number of iterations needed for 3 by 3 system\")\n",
    "num_iters = 1000\n",
    "for eps in [0.00001]: \n",
    "    A3 = torch.tensor([[1+eps,-1,0],[-1,2+eps,-1],[0,-1,1+eps]])\n",
    "    x = torch.zeros(3)\n",
    "    x = x.view(3,1)\n",
    "    b = torch.tensor([[-1.],[-1.],[2.]]) # must be in kernel is eps = 0\n",
    "    x.data = torch.tensor([[1.0],[2.],[3.0]])\n",
    "    tol = 1e-8 # tolerance for residual norm \n",
    "    residual_norm = torch.norm(torch.matmul(A3,x) -b,2)\n",
    "    iters = 0 \n",
    "    while residual_norm > tol: \n",
    "        gd = torch.matmul(A3,x) - b \n",
    "        x = x - 0.5*gd \n",
    "        residual_norm = torch.norm(gd,2)\n",
    "        iters += 1 \n",
    "        if iters > num_iters: \n",
    "            break\n",
    "        assert torch.isnan(residual_norm)!=True, \"norm is nan, reset learning rate\" #somehow nan>tol returns false\n",
    "    if iters > num_iters: \n",
    "        print(\"eps = \"+str(eps)+\": over {}\".format(num_iters))\n",
    "    else:\n",
    "        print(\"eps = \"+str(eps)+\": \", iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc85a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
