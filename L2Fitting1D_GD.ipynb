{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f19a679",
   "metadata": {},
   "source": [
    "###  Use the gradient descent method to train a ReLU a shallow neural network for L^2 fitting tasks\n",
    "\n",
    "Consider 1D  $L^2$-fitting problem on $\\Omega = [-\\pi,\\pi]$: \n",
    "\t\t\\begin{equation}\n",
    "\t\t\t\\min_{f_n \\in \\Sigma_n} \\int_{-\\pi}^\\pi \\frac{1}{2} | f(x) - f_n(x) |^2dx.\n",
    "\t\t\\end{equation}\n",
    "        \n",
    " $\\Sigma_n$  is the spcae of  ReLU shallow neural network with $n$ neurons \n",
    "\t\\begin{equation*}\n",
    "\t\t\\Sigma_n = \\left\\{ v(x) = \\sum_{i=1}^n a_i \\sigma(x + b_i) : a_i \\in \\mathbb{R},  ~~ b_i \\in \\mathbb{R}\\right\\}, \\quad \\sigma(x) = \\max(0,x).\n",
    "\t\\end{equation*}\n",
    "    \n",
    "The resulting nonlinear, nonconvex optimization problem\n",
    "\t\t\t\\begin{equation} \\label{snn-optimization}\n",
    "\t\t\t \\min_{a_i, b_i} \\int_{-\\pi}^{\\pi} \\frac{1}{2} |f(x) - \\sum_{i=1}^{n} a_i \\sigma( x + b_i) |^2 dx.  \n",
    "\t\t\\end{equation}\n",
    "\t\n",
    "The above optimization problem is usually solved by GD (or Adam). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "pi = torch.tensor(np.pi,dtype=torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\"  \n",
    "    \n",
    "# 5 Point Gauss Quadrature rule, defined as global variables \n",
    "gx = torch.tensor([-0.9061798459386639927976, -0.5384693101056830910363, 0, 0.5384693101056830910363, \n",
    "     0.9061798459386639927976]).to(device)\n",
    "gx = gx.view(1,-1)\n",
    "gw = torch.tensor([0.2369268850561890875143, 0.4786286704993664680413, 0.5688888888888888888889, 0.4786286704993664680413,\n",
    "     0.2369268850561890875143]).to(device)\n",
    "gw = gw.view(-1,1) # Column vector\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x)))\n",
    "        return u1\n",
    "\n",
    "def plot_l2_error_history(alg_name,hidden_size1,err): \n",
    "    plt.figure(dpi = 100)\n",
    "    plt.title(alg_name + ': neuron number '+str(hidden_size1))\n",
    "    plt.plot(err, label = 'L2 error', linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('error')\n",
    "    plt.show()\n",
    "\n",
    "def plot_solution(r1,r2,model,x_test,u_true,name=None): \n",
    "    # Plot function: test results \n",
    "    u_model_cpu = model(x_test).cpu().detach()\n",
    "    plt.figure(dpi = 100)\n",
    "    plt.plot(x_test.cpu(),u_model_cpu,label = \"nn function\")\n",
    "    plt.plot(x_test.cpu(),u_true.cpu(),label = \"true\")\n",
    "    if name!=None: \n",
    "        plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def compute_integration_nodes_relunn(a,b,model): \n",
    "\n",
    "    \"\"\"\n",
    "    input: \n",
    "        a,b: the interval [a,b]\n",
    "        model: nn model for extracting \n",
    "        weight, bias: \\sigma(weight*x + bias) is each neuron. both are 1D tensor\n",
    "    return: a column vector. size: (# cols)*(# row) \n",
    "    \"\"\"\n",
    "    weight = model.fc1.weight.detach().squeeze()\n",
    "    bias = model.fc1.bias.detach() \n",
    "    neuron_number = bias.size(0)\n",
    "    node = torch.empty(neuron_number + 2).to(device)\n",
    "    node[-1] = a\n",
    "    node[-2] = b \n",
    "    node[0:neuron_number] = - bias / weight \n",
    "    node = node[(node <= b)] \n",
    "    node = node[(node >= a)]\n",
    "    node = node.view(-1,1)\n",
    "    if neuron_number < 100: # too fewer subintervalus soemtimes lead to inaccurate GQ \n",
    "        refined_node = torch.linspace(a,b,100).view(-1,1)\n",
    "        node = torch.cat([node,refined_node])\n",
    "    node = node.unique()\n",
    "    node, indices = torch.sort(node)\n",
    "    node = node.view(-1,1)\n",
    "    return node\n",
    "\n",
    "def GQ_piecewise(gw,gx,nodes,func): \n",
    "    \"\"\"\n",
    "    Compute piecewise Gaussian quadrature for function func on subintervals determined by nodes\n",
    "    \"\"\"\n",
    "    n = len(nodes) - 1 \n",
    "    # 1. Get the transformed integration points for each subinterval, stored in rows\n",
    "    coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "    coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "    coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "    integration_points = coef1@gx + coef2_expand\n",
    "    integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "    \n",
    "    # 2. Function values on the integration points on each subinterval\n",
    "    func_values = func(integration_points)\n",
    "    # Modify gw, coef1 to be compatible with func_values\n",
    "    gw = torch.tile(gw,(n,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "    # Modify coef1 to be compatible with func_values\n",
    "    coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "    coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "    integral_values = torch.matmul(func_values.T,(gw*coef1_expand))\n",
    "    return integral_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdamOrGD_train(my_model, target,learning_rate,num_epochs,initialize_): \n",
    "    \"\"\"Training SNN using Adam or GD\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    my_model : nn model class \n",
    "        Shallow neural network model \n",
    "    target : function\n",
    "        the target function\n",
    "    learning_rate : float \n",
    "        learning rate for neural network training \n",
    "    num_epochs : int \n",
    "        number of epochs\n",
    "    initialize_ : function\n",
    "        initialize the nn parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    err : torch tensor\n",
    "        L2 error history with respect to the epoch number \n",
    "    my_model : nn model class \n",
    "        trained nn model\n",
    "    \"\"\"\n",
    "    \n",
    "    def loss_function_inside(x): # Integrand of the loss function\n",
    "        return 0.5*torch.pow(my_model(x)-target(x),2).to(device)\n",
    "\n",
    "    start = time.time()\n",
    "    hidden_size1 = my_model.fc1.bias.size(0)\n",
    "    print(\"current hidden layer size: \",hidden_size1)\n",
    "    if initialize_ != None: \n",
    "        initialize_(my_model,target)\n",
    "        \n",
    "    # Plot initial guess and compute initial error\n",
    "    x_test = torch.linspace(-pi,pi,4096).view(4096,1).to(device) \n",
    "    u_true = target(x_test)\n",
    "    plot_solution(-pi,pi,my_model,x_test,u_true)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        node = compute_integration_nodes_relunn(-pi,pi,my_model)\n",
    "        loss = GQ_piecewise(gw,gx,node,loss_function_inside)\n",
    "    print(\"original loss\", (2*loss)**0.5)\n",
    "    # Define variables for data to be stored \n",
    "    err = torch.zeros(num_epochs+1, 1)\n",
    "    err[0] = (2*loss)**0.5\n",
    "    \n",
    "    # Define optimizer \n",
    "    optimizer = optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Start the training process\n",
    "    lr = learning_rate  \n",
    "    for epoch in range(num_epochs):\n",
    "        # Plot the numerical solution \n",
    "        if (epoch+1) % 5000 == 0: \n",
    "            plot_solution(-pi,pi,my_model,x_test,u_true)\n",
    "          \n",
    "        lr = learning_rate \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr * (0.98 ** ((epoch + 1) // 1000)) # Learning rate schedule\n",
    "        optimizer.zero_grad() # Clear gradient \n",
    "        \n",
    "        # Define the loss function \n",
    "        node = compute_integration_nodes_relunn(-pi,pi,my_model)\n",
    "        loss = GQ_piecewise(gw,gx,node,loss_function_inside)\n",
    "        loss.backward() # Compute gradient \n",
    "        optimizer.step() # Gradient descent step\n",
    "        with torch.no_grad(): \n",
    "            node = compute_integration_nodes_relunn(-pi,pi,my_model)\n",
    "            loss = GQ_piecewise(gw,gx,node,loss_function_inside)\n",
    "        err[epoch+1] = torch.pow(loss.detach()*2.0,0.5)\n",
    "\n",
    "    # Plot L^2 error history and solution \n",
    "    plot_l2_error_history(\"Adam: \",hidden_size1,err)\n",
    "    plot_solution(-pi,pi,my_model,x_test,u_true)\n",
    "    end = time.time()\n",
    "    print(str(end - start)+\" s\")\n",
    "    return err, my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ebed3",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "1. $f(x) = sin(x)$\n",
    "\n",
    "2. $f(x) = sin(x) + 0.2*sin(10*x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0a19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66429727",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_model = model(1,20,1)\n",
    "def target(x):\n",
    "    return torch.sin(x)\n",
    "learning_rate = 0.01 \n",
    "initialize_ = None \n",
    "num_epochs = 10000\n",
    "err, my_model = AdamOrGD_train(my_model, target,learning_rate,num_epochs,initialize_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ff702",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_model = model(1,20,1)\n",
    "def target(x):\n",
    "    return torch.sin(x) + 0.2*torch.sin(10*x)\n",
    "learning_rate = 0.01 \n",
    "initialize_ = None \n",
    "num_epochs = 10000\n",
    "err, my_model = AdamOrGD_train(my_model, target,learning_rate,num_epochs,initialize_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0182f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def target(x):\n",
    "    return torch.sin(x) \n",
    "err_list = []\n",
    "for neuron_num in [8,16,32,64]: \n",
    "    my_model = model(1,neuron_num,1)\n",
    "    learning_rate = 0.01 \n",
    "    initialize_ = None \n",
    "    num_epochs = 10000\n",
    "    err, my_model = AdamOrGD_train(my_model, target,learning_rate,num_epochs,initialize_)\n",
    "    err_list.append(min(err))\n",
    "print(err_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc7e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def target(x):\n",
    "    return torch.sin(x) + 0.2*torch.sin(10*x)\n",
    "err_list = []\n",
    "for neuron_num in [8,16,32,64]: \n",
    "    my_model = model(1,neuron_num,1)\n",
    "    learning_rate = 0.01 \n",
    "    initialize_ = None \n",
    "    num_epochs = 10000\n",
    "    err, my_model = AdamOrGD_train(my_model, target,learning_rate,num_epochs,initialize_)\n",
    "    err_list.append(min(err))\n",
    "print(err_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dad9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
