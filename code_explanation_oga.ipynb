{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc9864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "pi = torch.tensor(np.pi,dtype=torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55344afc",
   "metadata": {},
   "source": [
    "## Orthogonal greedy algorithm for finite neuron method: $L^2$-fitting\n",
    "\n",
    "\n",
    "**OGA:**\n",
    "\\begin{equation} \n",
    "\t\t\tf_0 = 0, \\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle |, \\quad f_n = P_n(f),  \n",
    "\t\t\\end{equation}\n",
    "\t\twhere $P_n$ is a projection onto $H_n = \\text{span}\\{g_1, g_2,...,g_n \\}$\n",
    "\t\t For 1D $L^2$-fitting for target $f(x)$ with $f(0) = 0$, the ReLU shallow neural network dictionary can be given by \n",
    "\t\t\\begin{equation}\n",
    "\t\t\t\\mathbb{D} = \\{ \\sigma( x + b), b \\in [-\\pi,\\pi]  \\}, ~~~ \\sigma(x) = \\max(0,x)\n",
    "\t\t\\end{equation}\n",
    "        \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Theretically guaranteed convergence rate:** \n",
    "\n",
    "Let the iterates $f_n$ be given by the orthogonal greedy algorithm, where $f\\in \\mathcal{K}_1(\\mathbb{P}_k^d)$, where $\\mathbb{P}_k^d$ is the $\\text{ReLU}^k$ dictionary, $\\sigma_k(\\omega \\cdot x + b)$,  $\\sigma(x) = \\max^k(0,x)$\n",
    "\n",
    "Then we have\n",
    "  \\begin{equation}\n",
    "   \\|f_n - f\\|_{L^2} \\lesssim \\|f\\|_{\\mathcal{K}_1(\\mathbb{P}_k^d)}n^{-\\frac{1}{2} - \\frac{2k+1}{2d}}.\n",
    "  \\end{equation}\n",
    "  \n",
    " **In particular**, for a ReLU shallow neural network in 1D, we have ($k=1, d=1$)\n",
    "   \\begin{equation}\n",
    "   \\|f_n - f\\|_{L^2} \\lesssim \\|f\\|_{\\mathcal{K}_1(\\mathbb{P}_1^1)}n^{-2}.\n",
    "  \\end{equation}\n",
    "  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Two major steps in OGA**:\n",
    "\n",
    "1. argmax \n",
    "2. projection\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**How to implement the two major steps in OGA?**\n",
    "\n",
    "1. Solve the argmax problem $\\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle | $ \n",
    "    - $L^2$ innner product: numerical integration \n",
    "        - Piecewise Gauss quadrature (very accurate numerical quadrature is needed for this part)\n",
    "    - **Method of exhaustion** for 1D for a good initial guess \n",
    "        - $\\mathbb{D}_N = \\{ \\sigma(x + b_i), b_i = \\pi -  2 \\pi\\frac{i-1}{N} \\}_{i = 1}^N$\n",
    "        - $\\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D_N}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle | $ \n",
    "            - store values of $g$ as a row vector, values of $f - f_{n-1}$ column vector\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. ReLU dictionary discretization \n",
    "# relu dictionary\n",
    "def relu_dict(x_l,x_r,N):\n",
    "\n",
    "    relu_dict_parameters = torch.zeros((N,2))\n",
    "    relu_dict_parameters[:N,0] = torch.ones(N)[:]\n",
    "    relu_dict_parameters[:N,1] = torch.linspace(x_l,x_r,N+1)[1:] # relu(x+bi)\n",
    "\n",
    "    return relu_dict_parameters\n",
    "def target(x):\n",
    "    return x \n",
    "\n",
    "## 2. Method of exhaustion \n",
    "relu_dict_parameters = relu_dict(-pi,pi,100)\n",
    "integration_intervals = 10 \n",
    "nodes = torch.linspace(-pi,pi,integration_intervals+1).view(-1,1) \n",
    "coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "integration_points = coef1@gx + coef2_expand\n",
    "integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "gw_expand = torch.tile(gw,(integration_intervals,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "func_values = target(integration_points) \n",
    "\n",
    "weight_func_values = func_values*gw_expand*coef1_expand\n",
    "print(weight_func_values.size())\n",
    "basis_values = F.relu(relu_dict_parameters[:,0] *integration_points + relu_dict_parameters[:,1]).T # uses broadcasting  \n",
    "print(basis_values.size())\n",
    "output = torch.abs(torch.matmul(basis_values,weight_func_values)) # each component contains an integral value \n",
    "print(output.size())\n",
    "neuron_index = torch.argmax(output.flatten())\n",
    "\n",
    "print(neuron_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662aad66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ed85b6",
   "metadata": {},
   "source": [
    "2. Orthogonal projection. Given $H_n = \\text{span}\\{g_1, g_2,...,g_n \\}$\n",
    "    - Use w_list, b_list to keep track of the added neurons\n",
    "        - Construct the neural network from the two lists \n",
    "    - Orthogonal projection: Solve the linear system.\n",
    "         - $f_n = P_{H_m} f$  \n",
    "         - $<f_n,g_i> = <f,g_i>$ for all $i= 1,2,...,n$, with $f_n = \\sum_{i=1}^n a_i g_i $\n",
    "         - A linear system in $\\alpha := (a_1, a_2,...,a_n)^T$. $M \\alpha = b$, where $M_{i,j} = <g_j,g_i>$ $b_i = <f,g_i> $\n",
    "         - Equivalently, an $L^2$-fitting: $$\\min_{a_1, a_2,...,a_n} \\frac{1}{2} \\|\\sum_{i=1}^n a_i g_i  - f \\|^2_{L^2}, $$ that is, $$\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2} \\alpha^T M \\alpha - b^T \\alpha $$\n",
    "    - A shortcut: make use of pytorch automatic differentiation to get the matrix M, since we already know how to compute the loss function very accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0eb69",
   "metadata": {},
   "source": [
    "#### Extract the linear system using pytorch auto differentiation and solve the linear system \n",
    "\n",
    "$L(x) = \\frac{1}{2}x^T A x$\n",
    "\n",
    "$\\nabla_x L(x) = A x$\n",
    "\n",
    "$\\nabla_x (Ax)_i = \\text{ ith row of } A $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce450155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_linear_layer(model,target,solver=\"cg\"):\n",
    "    \"\"\"Solve the linear layer problem Mx = b: L2 fitting relu shallow neural networks \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "        relu shallow neural network\n",
    "    target: \n",
    "        a target function \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sol: tensor \n",
    "        the solution of the linear system \n",
    "    \"\"\"\n",
    "    def loss_function_inside(x):\n",
    "        return 0.5*torch.pow(model(x)-target(x),2).to(device)\n",
    "\n",
    "    def rhs_loss_inside(x): \n",
    "        return model(x)*target(x)\n",
    "    #1. Compute loss function using piecewise Gaussian quadrature\n",
    "    node = compute_integration_nodes_relunn(-pi,pi,model)\n",
    "    loss = GQ_piecewise(gw,gx,node,loss_function_inside) #loss_function\n",
    "\n",
    "    #2. Extract the linear system A using torch.autograd \n",
    "    du1 = torch.autograd.grad(outputs=loss, inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "    jac = '' \n",
    "    neuron_number = model.fc1.bias.size(0)\n",
    "    for i in range(neuron_number): \n",
    "        duui = torch.autograd.grad(outputs=du1[0,i], inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "        if i == 0: \n",
    "            jac = duui\n",
    "        else: \n",
    "            jac = torch.cat([jac,duui],dim=0)\n",
    "\n",
    "    #3. Extract the right hand side\n",
    "    loss_helper = GQ_piecewise(gw,gx,node,rhs_loss_inside)\n",
    "    rhs = torch.autograd.grad(outputs=loss_helper, inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "    rhs = rhs.view(-1,1)\n",
    "\n",
    "    #4. Solve the linear system \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach()),np.array(rhs.detach()),tol=1e-10) \n",
    "    elif solver == \"direct\": \n",
    "        sol = np.linalg.inv( np.array(jac.detach()) )@np.array(rhs.detach())\n",
    "    sol = torch.tensor(sol).view(1,-1)\n",
    "    return sol \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a507821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a08b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
