{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74f4f50",
   "metadata": {},
   "source": [
    "### Our approach: orthogonal greedy algorithm \n",
    "\n",
    "**OGA:**\n",
    "\\begin{equation} \n",
    "\t\t\tf_0 = 0, \\quad g_n  = \\mathop{\\arg \\max}_{g \\in \\mathbb{D}}  | \\langle g, f - f_{n-1}\n",
    "\t\t\t\\rangle |, \\quad f_n = P_n(f),  \n",
    "\t\t\\end{equation}\n",
    "\t\twhere $P_n$ is a projection onto $H_n = \\text{span}\\{g_1, g_2,...,g_n \\}$\n",
    "\t\t For 1D $L^2$-fitting for target $f(x)$ with $f(0) = 0$, the ReLU shallow neural network dictionary can be given by \n",
    "\t\t\\begin{equation}\n",
    "\t\t\t\\mathbb{D} = \\{ \\sigma( x + b), b \\in [-\\pi,\\pi]  \\}, ~~~ \\sigma(x) = \\max(0,x)\n",
    "\t\t\\end{equation}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9f495",
   "metadata": {},
   "source": [
    "**Theretically guaranteed convergence rate:** \n",
    "\n",
    "Let the iterates $f_n$ be given by the orthogonal greedy algorithm, where $f\\in \\mathcal{V}_1(\\mathbb{P}_k^d)$, where $\\mathbb{P}_k^d$ is the $\\text{ReLU}^k$ dictionary, $\\sigma_k(\\omega \\cdot x + b)$,  $\\sigma(x) = \\max^k(0,x)$\n",
    "\n",
    "Then we have\n",
    "  \\begin{equation}\n",
    "   \\|f_n - f\\|_{L^2} \\lesssim \\|f\\|_{\\mathcal{V}_1(\\mathbb{P}_k^d)}n^{-\\frac{1}{2} - \\frac{2k+1}{2d}}.\n",
    "  \\end{equation}\n",
    "  \n",
    " **In particular**, for a ReLU shallow neural network in 1D, we have ($k=1, d=1$)\n",
    "   \\begin{equation}\n",
    "   \\|f_n - f\\|_{L^2} \\lesssim \\|f\\|_{\\mathcal{V}_1(\\mathbb{P}_1^1)}n^{-2}.\n",
    "  \\end{equation}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b562f6",
   "metadata": {},
   "source": [
    "**Two major steps in OGA**:\n",
    "\n",
    "1. argmax \n",
    "2. projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bad8f",
   "metadata": {},
   "source": [
    "**How to implement the two major steps in OGA?**\n",
    "\n",
    "1. Solve the argmax problem\n",
    "    - $L^2$ innner product: numerical integration \n",
    "        - Piecewise Gauss quadrature (very accurate numerical quadrature is needed for this part)\n",
    "    - Method of exhaustion for 1D for a good initial guess \n",
    "        - $\\mathbb{D}_N = \\{ \\sigma(x + b_i), b_i =\\pi -  2 \\pi\\frac{i-1}{N} \\}_{i = 1}^N$\n",
    "    - Further optimization using Newton's method to get a better $g_n$ (This part is also essential when the number of neuron is large)\n",
    "    \n",
    "2. Orthogonal projection. Given $H_n = \\text{span}\\{g_1, g_2,...,g_n \\}$\n",
    "    - Some mathematics derivation here for the linear system\n",
    "         - $<f_n,g_i> = <f,g_i>$ for all $i= 1,2,...,n$, with $f_n = \\sum_{i=1}^n a_i g_i $\n",
    "         - A liner system in $\\alpha := (a_1, a_2,...,a_n)^T$. $M \\alpha = b$, where $M_{i,j} = <g_j,g_i>$ $b_i = <f,g_i> $\n",
    "         - Equivalently, an $L^2$-fitting: $\\min_{a_1, a_2,...,a_n} \\frac{1}{2} \\|\\sum_{i=1}^n a_i g_i  - f \\|^2_{L^2} $, that is $\\min_{\\alpha \\in \\mathbb{R}^n} \\frac{1}{2} \\alpha^T M \\alpha - b^T \\alpha $\n",
    "    - A shortcut: make use of pytorch automatic differentiation to get the matrix M, since we already know how to compute the loss function very accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "pi = torch.tensor(np.pi,dtype=torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\"  \n",
    "    \n",
    "# 5 Point Gauss Quadrature rule, defined as global variables \n",
    "gx = torch.tensor([-0.9061798459386639927976, -0.5384693101056830910363, 0, 0.5384693101056830910363, \n",
    "     0.9061798459386639927976]).to(device)\n",
    "gx = gx.view(1,-1)\n",
    "gw = torch.tensor([0.2369268850561890875143, 0.4786286704993664680413, 0.5688888888888888888889, 0.4786286704993664680413,\n",
    "     0.2369268850561890875143]).to(device)\n",
    "gw = gw.view(-1,1) # Column vector\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x)))\n",
    "        return u1\n",
    "\n",
    "def plot_solution_modified(r1,r2,model,x_test,u_true,name=None): \n",
    "    # Plot function: test results \n",
    "    u_model_cpu = model(x_test).cpu().detach()\n",
    "    \n",
    "    w = model.fc1.weight.data.squeeze()\n",
    "    b = model.fc1.bias.data.squeeze()\n",
    "    x_model_pt = (-b/w).view(-1,1)\n",
    "    u_model_pt = model(x_model_pt).cpu().detach()\n",
    "    plt.figure(dpi = 100)\n",
    "    plt.plot(x_test.cpu(),u_model_cpu,'-.',label = \"nn function\")\n",
    "    plt.plot(x_test.cpu(),u_true.cpu(),label = \"true\")\n",
    "    if name!=None: \n",
    "        plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def compute_integration_nodes_relunn(a,b,model): \n",
    "\n",
    "    \"\"\"\n",
    "    input: \n",
    "        a,b: the interval [a,b]\n",
    "        model: nn model for extracting \n",
    "        weight, bias: \\sigma(weight*x + bias) is each neuron. both are 1D tensor\n",
    "    return: a column vector. size: (# cols)*(# row) \n",
    "    \"\"\"\n",
    "    weight = model.fc1.weight.detach().squeeze()\n",
    "    bias = model.fc1.bias.detach() \n",
    "    neuron_number = bias.size(0)\n",
    "    node = torch.empty(neuron_number + 2).to(device)\n",
    "    node[-1] = a\n",
    "    node[-2] = b \n",
    "    node[0:neuron_number] = - bias / weight \n",
    "    node = node[(node <= b)] \n",
    "    node = node[(node >= a)]\n",
    "    node = node.view(-1,1)\n",
    "    if neuron_number < 100: # too fewer subintervalus soemtimes lead to inaccurate GQ \n",
    "        refined_node = torch.linspace(a,b,100).view(-1,1)\n",
    "        node = torch.cat([node,refined_node])\n",
    "    node = node.unique()\n",
    "    node, indices = torch.sort(node)\n",
    "    node = node.view(-1,1)\n",
    "    return node\n",
    "\n",
    "def GQ_piecewise(gw,gx,nodes,func): \n",
    "    \"\"\"\n",
    "    Compute piecewise Gaussian quadrature for function func on subintervals determined by nodes\n",
    "    \"\"\"\n",
    "    n = len(nodes) - 1 \n",
    "    # 1. Get the transformed integration points for each subinterval, stored in rows\n",
    "    coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "    coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "    coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "    integration_points = coef1@gx + coef2_expand\n",
    "    integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "    \n",
    "    # 2. Function values on the integration points on each subinterval\n",
    "    func_values = func(integration_points)\n",
    "    # Modify gw, coef1 to be compatible with func_values\n",
    "    gw = torch.tile(gw,(n,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "    # Modify coef1 to be compatible with func_values\n",
    "    coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "    coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "    integral_values = torch.matmul(func_values.T,(gw*coef1_expand))\n",
    "    return integral_values\n",
    "\n",
    "\n",
    "def minimize_linear_layer(model,target,solver=\"cg\"):\n",
    "    \"\"\"Solve the linear layer problem Mx = b: L2 fitting relu shallow neural networks \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "        relu shallow neural network\n",
    "    target: \n",
    "        a target function \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sol: tensor \n",
    "        the solution of the linear system \n",
    "    \"\"\"\n",
    "    def loss_function_inside(x):\n",
    "        return 0.5*torch.pow(model(x)-target(x),2).to(device)\n",
    "\n",
    "    def rhs_loss_inside(x): \n",
    "        return model(x)*target(x)\n",
    "    #1. Compute loss function using piecewise Gaussian quadrature\n",
    "    node = compute_integration_nodes_relunn(-pi,pi,model)\n",
    "    loss = GQ_piecewise(gw,gx,node,loss_function_inside) #loss_function\n",
    "\n",
    "    #2. Extract the linear system A using torch.autograd \n",
    "    du1 = torch.autograd.grad(outputs=loss, inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "    jac = '' \n",
    "    neuron_number = model.fc1.bias.size(0)\n",
    "    for i in range(neuron_number): \n",
    "        duui = torch.autograd.grad(outputs=du1[0,i], inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "        if i == 0: \n",
    "            jac = duui\n",
    "        else: \n",
    "            jac = torch.cat([jac,duui],dim=0)\n",
    "    #3. Extract the right hand side\n",
    "    loss_helper = GQ_piecewise(gw,gx,node,rhs_loss_inside)\n",
    "    rhs = torch.autograd.grad(outputs=loss_helper, inputs=model.fc2.weight, retain_graph=True,create_graph = True)[0]\n",
    "    rhs = rhs.view(-1,1)\n",
    "\n",
    "    #4. Solve the linear system \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach()),np.array(rhs.detach()),tol=1e-12) \n",
    "    elif solver == \"direct\": \n",
    "        sol = np.linalg.inv( np.array(jac.detach()) )@np.array(rhs.detach())\n",
    "    sol = torch.tensor(sol).view(1,-1)\n",
    "    return sol \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b30bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu dictionary\n",
    "def relu_dict(x_l,x_r,N):\n",
    "    \"\"\"generate relu dictionary parameters \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_l: float \n",
    "    x_r: float\n",
    "    N: int \n",
    "        number of degree of freedoms\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch tensor\n",
    "        containing relu dictionary parameters, corresponds to nodal points\n",
    "        \n",
    "    \"\"\"\n",
    "    # w = 1 \n",
    "    relu_dict_parameters = torch.zeros((N,2))\n",
    "    relu_dict_parameters[:N,0] = torch.ones(N)[:]\n",
    "    relu_dict_parameters[:N,1] = torch.linspace(x_l,x_r,N+1)[1:] # relu(x+bi)\n",
    "\n",
    "    return relu_dict_parameters\n",
    "\n",
    "def OGAL2FittingReLU1D(my_model,target,relu_dict_parameters,num_epochs,plot_freq = 10,integration_intervals=1000): \n",
    "    \"\"\" Orthogonal greedy algorithm using 1D ReLU dictionary over [-pi,pi]\n",
    "    Parameters\n",
    "    ----------\n",
    "    my_model: \n",
    "        nn model \n",
    "    target: \n",
    "        target function\n",
    "    num_epochs: int \n",
    "        number of training epochs \n",
    "    integration_intervals: int \n",
    "        number of subintervals for piecewise numerical quadrature \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    err: tensor \n",
    "        rank 1 torch tensor to record the L2 error history  \n",
    "    model: \n",
    "        trained nn model \n",
    "    \"\"\"\n",
    "    nodes = torch.linspace(-pi,pi,integration_intervals+1).view(-1,1) \n",
    "    coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "    coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "    coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "    integration_points = coef1@gx + coef2_expand\n",
    "    integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "    gw_expand = torch.tile(gw,(integration_intervals,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "    coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "    coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "\n",
    "    num_neuron = 0\n",
    "    list_b = []\n",
    "    list_w = []\n",
    "    err = torch.zeros(num_epochs+1)\n",
    "    func_values = target(integration_points)\n",
    "    func_values_sqrd = func_values*func_values\n",
    "    err[0]= torch.sum(func_values_sqrd*gw_expand*coef1_expand)**0.5\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(num_epochs): \n",
    "        if num_neuron == 0: \n",
    "            func_values = target(integration_points)\n",
    "        else: \n",
    "            func_values = target(integration_points) - my_model(integration_points).detach()\n",
    "\n",
    "        weight_func_values = func_values*gw_expand*coef1_expand\n",
    "        basis_values = F.relu(relu_dict_parameters[:,0] *integration_points + relu_dict_parameters[:,1]).T # uses broadcasting  \n",
    "        output = torch.abs(torch.matmul(basis_values,weight_func_values)) # \n",
    "        neuron_index = torch.argmax(output.flatten())\n",
    "\n",
    "        list_w.append(relu_dict_parameters[neuron_index,0])\n",
    "        list_b.append(relu_dict_parameters[neuron_index,1])\n",
    "        num_neuron += 1\n",
    "        my_model = model(1,num_neuron,1)\n",
    "        my_model.fc1.weight.data[:,0] = torch.tensor(list_w)[:]\n",
    "        my_model.fc1.bias.data[:] = torch.tensor(list_b)[:]\n",
    "        sol = minimize_linear_layer(my_model,target,\"cg\")\n",
    "        my_model.fc2.weight.data[0,:] = sol[:]\n",
    "        if (i+1)%plot_freq == 0: \n",
    "            x_test = torch.linspace(-pi,pi,200).view(-1,1)\n",
    "            u_true = target(x_test)\n",
    "            plot_solution_modified(-pi,pi,my_model,x_test,u_true)\n",
    "        func_values = target(integration_points) - my_model(integration_points).detach()\n",
    "        func_values_sqrd = func_values*func_values\n",
    "        err[i+1]= torch.sum(func_values_sqrd*gw_expand*coef1_expand)**0.5\n",
    "    print(\"time taken: \",time.time() - start_time)\n",
    "    return err, my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5344c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e04761cb",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "\n",
    "1. $f(x) = sin(x)$\n",
    "\n",
    "2. $f(x) = sin(x) + 0.2*sin(10*x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2080d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93654ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(x):\n",
    "    return torch.sin(x) \n",
    "relu_dict_parameters = relu_dict(-pi,pi,2**10) # \n",
    "err_relu_list = [] \n",
    "my_model = None \n",
    "argmax_sub= False \n",
    "for num_epochs in [260]: \n",
    "    err, my_model = OGAL2FittingReLU1D(my_model,target,relu_dict_parameters, \\\n",
    "                    num_epochs,plot_freq = num_epochs,integration_intervals=2**10)\n",
    "for i in [2,4,8,16,32,64,128,256]: \n",
    "    print(err[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf090a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(x):\n",
    "    return torch.sin(x) + 0.2*torch.sin(10*x)\n",
    "relu_dict_parameters = relu_dict(-pi,pi,2**10) # \n",
    "err_relu_list = [] \n",
    "my_model = None \n",
    "argmax_sub= False \n",
    "for num_epochs in [260]: \n",
    "    err, my_model = OGAL2FittingReLU1D(my_model,target,relu_dict_parameters, \\\n",
    "                    num_epochs,plot_freq = num_epochs,integration_intervals=2**10)\n",
    "for i in [2,4,8,16,32,64,128,256]: \n",
    "    print(err[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4f2975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
